{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# tensorflow: Training Neural Networks for Classification and Dimensionality Reduction\n",
    "\n",
    "This notebook demonstrates example neural network setups for **image classification** and **dimensionality reduction** tasks using `tensorflow`.\n",
    "\n",
    "The examples serve as complement and comparison for exploring neural networks with the `edu.yaprnn` app.\n",
    "\n",
    "### TensorFlow on Windows: Virtual Environment Setup\n",
    "\n",
    "To ensure TensorFlow compatibility on Windows, use the script `setup-venv-tensorflow-2.10.1.sh` to create a dedicated Python virtual environment with TensorFlow version `2.10.1`."
   ],
   "id": "b416c898b4c865ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T00:22:59.715183Z",
     "start_time": "2025-01-07T00:22:53.755167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mnist_utils\n",
    "import model_utils\n",
    "\n",
    "mnist_images, image_size, X, random_x_indices, random_feature_indices, y, labels = mnist_utils.from_mnist_images()\n",
    "\n",
    "digits_train_32k_test_12k = train_test_split(X, y, labels, train_size=0.6, test_size=0.2, random_state=model_utils.SEED)\n",
    "digits_images_from_labels = train_test_split(X, y, labels, train_size=0.8, test_size=0.2, random_state=model_utils.SEED)\n",
    "digits_input_reconstruction = train_test_split(X, y, labels, train_size=0.8, test_size=0.2,\n",
    "                                               random_state=model_utils.SEED)\n",
    "\n",
    "mnist_utils.from_dataset(digits_train_32k_test_12k, explain=True)\n",
    "mnist_utils.from_dataset(digits_images_from_labels, explain=True)\n",
    "mnist_utils.from_dataset(digits_input_reconstruction, explain=True)\n",
    "\n",
    "None"
   ],
   "id": "ddee58bbed8455ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST images from E:\\edu.yaprnn\\src\\main\\resources\\digits.idx3-ubyte\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAABZCAYAAAAjHfhhAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEYZJREFUeJztnQuUVdMbwHc1PUalPCKFpoghDa3UhFFMljwqovJO3pk8skreYhGSsDyWQvLIq9XQysIkimiULDWpGJoeJFQqrymvjvX71n/f/7nn3jtz5zb33n3H91vrdLtnzjn3PPa3v9fe36nneZ5nFEVJK/XT+/OKooAKoqI4gAqiojiACqKiOIAKoqI4gAqiojiACqKiOIAKoqI4gAqiojiACmICPPvss6ZevXpmzZo1oXXHHXecLIkwdOhQk5OTYzIFrpvrf//991P6u3fccYf87qZNm2rtmK7c+/qJNMAmTZqY7777LuLvNMTDDjusNs/vP0llZaU0umQ29D/++MPccMMNpk2bNiY7O9vk5+eb2bNn1/rv1PU2kZOTIzIRXIYNG1aj42Ql+hDvu+8+8+ijjyaye53knXfeSXjfp556yuzYsSNMEO+88075f6JaNh5NMH36dDNixAjTsWNH6WRPOeUUM3fuXFNQUJCU36yrHHHEEWbkyJFh6w466KDkCyI/TOO56aabpEdVjGnUqFHC+zZs2NCkkk8++cS88sorZvz48WbUqFGybsiQIaK5Ro8ebUpLS1N6PplO27Ztzfnnn596H/Hmm282//zzj2jF6pgyZYopLCw0e+21l2ncuLE59NBDzRNPPBFVxfft21fMsSOPPFLMpc6dO4fMs9dee02+YxZ37drVLF68OOIYX375pRk4cKDZfffdZTuOM3PmzIjtKioqZImH5cuXy/lzPvvuu6+5++67w7RXVT7i2rVrTf/+/U3Tpk3l+q+77joza9asCP/K76fgf7Vq1Ur+j1a0pg6mKvzwww/moosuknPhfu6zzz7mtNNOC/NXqwNN2KBBA3P55ZeH1nG/LrnkEvPxxx+bb7/91qSSpUuXyj3o0KGDnEfr1q3NxRdfbH766aeo2+MjDh482Oy6665mjz32MNdee63Zvn17xHZTp06VtsKzo02cffbZcV3b999/L23pr7/+ivsa/vzzT/P777+bRElII7Zv3156ULTijTfeWKVWROg6deokDTIrK8u88cYbpqioSBrz8OHDw7ZduXKlOffcc80VV1whPcwDDzxg+vXrZyZOnCjCz35w7733yoMoLy839evXDwnMMcccI70T50TjnzZtmjn99NNNcXGxGTBgQOh3evfuLZ/VNV4a/fHHH2/+/vvv0DGffPJJebDVwUNBgHmoNBQa10svvSSmX1UghNyzK6+8Us75jDPOkPV5eXnyeeaZZ8q1Xn311SK8GzZsEN/um2++iTvoQCeG6URD9tO9e3f5XLJkidlvv/1Mqpg9e7ZZtWqVdDDcJ66P+8znggULpCPyw7PnWmkH/P2RRx4xW7ZsMc8//3xom7Fjx5rbbrtNtr300kvNxo0bxZXq2bOnXH/Lli1jng+W3nPPPWdWr14d1z2dM2eO2WWXXUQ5tWvXTjpcnnmN8GrAlClTmLvoLVq0yKuoqPCysrK8a665JvT3Xr16eZ06dQrbp7KyMuI4ffr08Tp06BC2rl27dnLs0tLS0LpZs2bJuuzsbG/t2rWh9ZMmTZL1c+fODa3r3bu317lzZ2/79u2hdTt27PCOPvpor2PHjhG/xVIdI0aMkN9ZuHBhaN2GDRu8Fi1ayPrVq1eHXTuLZcKECbLNjBkzQuu2bdvm5ebmRpz7hRdeGHY+GzdulG3GjBkTdj5btmyR9ePHj/d2Bp5RYWFhxPrly5fL8SdOnFjl/lx38BpiEa1NBKmM0kZefvll+Y158+aF1nE/WNe/f/+wbYuKimR9WVmZfF+zZo3XoEEDb+zYsWHbff7559Jm/euD996uCz7fWPTr188bN26cPOfJkyd7xx57rOw7evRoryYknL7AjLjggguk56LXj4Vfe/z8889iVvTq1Ut6QL77wWw96qijQt+J5AGaZf/9949YzzFg8+bN0ivR+/3666/yGyyYNn369DFff/11WJQXTRiPKffWW2+ZHj16hDSF1VjnnXdetfuWlJSIdsYSsGB2XXbZZSZRuJf4opi1aIBE2bZtm5i1QTg/+/dUku1rI5iYPDvuO3z22WcR2wctKawD+7ysG4PFRXuwbYEFbUtgqjqrhMAV8+Xj0Ya4PvjVuAeY0x988IG0uQcffNCsW7cuNXnEW2+9Vcy2qnzF+fPnmxNOOEHMOswBGjJmJgQF0S9s0KJFC/kMmkl2vW2MmLTcOEwRju9fxowZI9tgwtUUfDweXJCDDz44rn0POOCACLPqwAMPNImC8IwbN868/fbbZu+99xYz6/777xcTuqYNn8h3EOtnxWN61yabN28WU45r4rd5brg/0doIBJ8J9xkXxXaudLy0B7YLtocvvvgiobYQLzxvTFPkoibpp4R8RL9WxJdDK+JDBSEggj+Wm5srPQQCRY9Oz/XQQw9FBD0IIEQj1npb5cMehwggvVE0dkYAXIJ0A37zjBkzJPBD54OvhEXQpUuXuI5BgCdaHthaNqmOhA8ePFgitddff71E5Js1aybP9KSTTooaGAsS7OzYh3V0WNHaDsdPJlZx0MGkRBCtViQ6RU8dhMAMPS/q26/tqjMNEukQbBoA7Vtb4HjTuwYhSBTPvitWrJDOwt9Q0N41bVhB0ADkrVg4PxrvhAkT5DnEA9vzDH755ZewgM3ChQtDf08VWDXvvfeeRIhvv/320Ppo993/N6sx7T1F+Kwpyf3hvrNNTfN5tYF1mWz0OyVD3LhotOKkSZMiTCTbG/nrU2FqkNKoTUgNkDrgHKL5q0TMEklfkOAmKkfezX+sF198sdp90cxoHX/6BNOPSHN1EIGDrVu3hq0n0R8M03P/mzdvHtXUjAUpHiJ8WDIW9ue54H+nMmIarY3Aww8/bGLx+OOPh323A0tOPvlk+STSzHER7uBx+R4rLVLT9AUaj/voh31w1bD8iLinTCPCLbfcYl544QXRFKQqLCeeeKKcEKYUKYnffvtNGiKCU1WAJxF4OIwIIddIQAQt+eOPP0peDKe5rKysxukLnHCuCxMJH8amL9B25L6qgut97LHHzDnnnCP7Yg4iwDYgUpXWw08icPXqq69Kj04OjGQ7fgfnjinH30kHvf7663Kd5MjiBWEbNGiQhOnxlzDbCddzPyZPnmxqGzov8q9B0FgEvnr+z9elERPgYpQSqYNY8DeCYDwXni+WAGmvww8/PNQ58XtcH9dECovOiv24X+RP7UCGnUlf0MnyO3RsXAuCSYpq2bJl5p577pHgUNwkmr4IYkO+wVD1zJkzvby8PK9JkyZeTk6OhHqfeeaZiPAwIeRTTz014rhsN3z48Kjh82AYn5TKkCFDvNatW3sNGzb02rZt6/Xt29ebPn16QukLWLp0qYTgOX+Od9ddd0mYurr0BaxatUquifRLq1atvJEjR3rFxcWy74IFC8LuXfB8SON07drVa9SoUSiVsWnTJrkXpECaNm0qaZT8/Hxv2rRpXk0hlTJq1Ci5V40bN/a6devmlZSUxLVvTdMXbBttIeUE69at8wYMGOC1bNlSrmnQoEHe+vXrI1I4Nn2xYsUKb+DAgV7z5s293XbbzbvqqqvkeoJwrwsKCuResXDfuH/l5eW1kr749NNPJX1Bu+A5NWvWTH4vkedRj3/iF1tlZ8HkIqqGlqb3z0TQMmgA/MxkjYX9r6HToJJIMB+Hf4cfS1g9U4VQSQ614iMq0SFoQLSYKCRBKnwZggDxBHsSAR+cpSqI5MVKBynpQwUxiRA5ffrpp0XwiK4RYGHWw1lnnZWU32Nsrp0+FYt4x08qqUV9xDoE+Subw4oFkWUbuVXcQQVRURxAgzWK4gDqI6aI6oatuY4aTslFNaKiOIAKoqI4gAqiojiACqKiOIAKoqI4gAqiojiACqKiOIDmETM8N8nkYFvb1ULx4T333FMGd1M+hEmxTLuixAQTt5mYzXhT9qPEBDVRmahNHVGqACipRwUxQ0GIEERm8wdL9lOlgNnqVH2j7AalLyjzx0x5qiZQ/ZoB6VRPoEI1BX4pCUIVbBXE9KCC6DAICsWd0GwslOigJCXaDCFjQcNRdt4P39kPYWVEjK29QskNimuxP5qQ0g7UGWJqFoWuUl3PVPk/KoiOgsmJSYnw8X+EknqqzCc85JBDZAYF2hDtRj1Qf7U4/3A0JiMzox4Bpo4Pn2hQtCNzJDFL0YSYptTEUdKDzr5wcKwpgkcBJN7lQQErW+EbAULLWVOUY7I+2kRfHisLgsa7IZgwjMazb62i8jVCSklF/Ee+ozljNQdtJslFNaKjoJ0QNKqaoxnRgLGEGTMTgWLyMftRQJftWc/LcKhuje9nNZ7dHv8QYUQz8n8lfaggOggCRe1NfDje5YEvWFUZfASMCcEIG4Jna+KwnmPMmzcv6mvLFHdQQXQQzECECt+O0vr4dtTIpAYp2o7KaZijtlgu66lHitBhflInhzqqRE4pchxP2XolvaggOgqmIgEUtBn+G8JFHpBgDW/MQkviL1KynhfevPnmmyKQVOwmeEOuEJ8PgVb/zn1UEB0GAfzwww+lqjj5QHw5TE4COUROqf6NoPJKAATS+nloSXxAqpKjDTUa6j4qiA6DACF8mJtEUvHziHqi+Rg9A2g7hM1GSe1+LJoXzBx0rGkGgKbDxETgCOQQkLHar1u3bpKkJ5iT6eU4/suoRswwyAfy+jReiMPLY2zSHx8SQQy++UrJDFQQMwx8P8aG4jPiI7IwnI23RqEx8Ss1OJN56MiaFFGbZiOpC1IaDO7mhTZUEF+0aJGkKgjsoBVJZfDKsmivvk4EbSbJRTViBoKfyGBt0hMMYWN2BcJIrhFTFYEk9fHVV19J0IYgjw3oKG6iGjFF1HYgheORR+SttMzAQDMyoBttiaCST+SlsMyq4PVpjNCp7gU1VaHNJLmoRsxQEAwEDo0IpaWlohltAIcoal5ennzyRmEGdpP4J9qqI23cQzViikhmaoHZGAggr38bOnSoTJeyeUb8RV4xjaAS5MF3TGTcqTaT5KIasQ75jIsXLxaBwV8kmMNsfMamMiQOs5Whb8XFxaaioiLdp6wEUEGsA2BqMsQNP5AgDSYo6YwePXrIrHwEk+lUzNyfP3++vCNRzVO30JE1dQgipARkSGG8++67krqwAsdgcUxXFnKPwTo3SnpRjVhHYCIw41AxRfkkYOOfuY+PyncdBucmKoh1AAaE22FumKNET5mPyHdbapEROevXr5d0BmUT8SsVd1BBzFBsqcT27duL2dmlSxdJ6BMxxS9k2BvbAAEc0hb4kJq+cBMVxAwFISQKSkKfYAyz9gnIEB3F/LTpBjtNyhaKQhA1FeEeKogZBH4fgkbAJTc3V7Qg0VGioVT29gdgEDaS+KQ15syZY1auXCkmKcPeFPdQQXQcfDxbYBgNiO+HIDLgu3v37iKAjJ6xE4MZf8rwNjQgaQqWjz76SIpLlZeXp/tylBioIGZApW/qz7AMGzZMymSQrOdv/pqmmKOkKxA4ymssWbJERtMwksZfSlFxExVEx0CwCLIQdEEICcSgCfnEDEUj4gtaP5C8IYKGGcqYUmqYIoQUmiK5r8WjMgMVRMfAzET7FRUVSfqBqCg+IMIXDQSQkvlTp04VM5RhbpinqgEzCxXENIN5SdoBoUPjUaUNvy8/P180Iou/yretW4PwEYBhihNjR5ctWxaq2K3picxDBTHNEOls06aNREALCwtlFgV5QExR+zYnBMu+0QkhtMWHmYFfUlIiZijBGTVBMxcVRAdSEgUFBaIBe/bsKf6hffkoAkfpRDSfrdhNTRqEj9qlLFu3blUhrAOoIKYZNBwBFytwfvyCyPA0BBHhKysrk5qlpCmUuoFODE4RVQ22thow+Ho1mxu05qldl45xotpMkosKYorI9FkP2kySi85HVBQHUEFUFAdQQVQUB9CoaYpQH0upCtWIiuIAKoiK4gAqiIriACqIiuIAKoiK4gAqiIriACqIiuIAKoiK4gAqiIpi0s+/jRRiaa0ihPUAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 784) (36000, 10) (36000,) (12000, 784) (12000, 10) (12000,)\n",
      "(48000, 784) (48000, 10) (48000,) (12000, 784) (12000, 10) (12000,)\n",
      "(48000, 784) (48000, 10) (48000,) (12000, 784) (12000, 10) (12000,)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T00:23:12.570784Z",
     "start_time": "2025-01-07T00:23:01.535405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def ssim_metric(y_true, y_pred):\n",
    "    y_true_reshaped = tf.reshape(y_true, [-1, image_size[0], image_size[1], 1])\n",
    "    y_pred_reshaped = tf.reshape(y_pred, [-1, image_size[0], image_size[1], 1])\n",
    "    return tf.reduce_mean(tf.image.ssim(y_true_reshaped, y_pred_reshaped, max_val=1.0))\n",
    "\n",
    "\n",
    "tf.config.list_logical_devices()"
   ],
   "id": "38f49be67646c836",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Digits Classification\n",
    "\n",
    "This model is configured as follows:\n",
    "\n",
    "- **Architecture**:\n",
    "  - Input layer: 784 neurons\n",
    "  - 2 hidden layers:\n",
    "    - Layer 1: 20 neurons\n",
    "    - Layer 2: 15 neurons\n",
    "  - Output layer: 10 neurons\n",
    "  - Total parameters: `((784 + 1) * 20) + ((20 + 1) * 15) + ((15 + 1) * 10) = 16175` (Biases included)\n",
    "- **Activation Function**: gelu\n",
    "- **Optimizer**: Stochastic Gradient Descent (SGD)\n",
    "  - Learning rate: 0.02\n",
    "  - Momentum: 0.2 with Nesterov term\n",
    "  - L1-weight-decay: 0.1 * 0.001 (bigger value breaks training)\n",
    "  - L2-weight-decay: 0.001\n",
    "- **Loss Function**: Mean Squared Error (MSE)"
   ],
   "id": "d281de62463487e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_size, output_size, X_train, y_train, X_test, y_test = mnist_utils.from_dataset(digits_train_32k_test_12k)\n",
    "\n",
    "# val_accuracy: 0.6867\n",
    "classification_batch_optimizer = SGD(learning_rate=model_utils.BATCH_LEARNING_RATE, momentum=model_utils.MOMENTUM,\n",
    "                                     nesterov=model_utils.NESTEROV_MOMENTUM)\n",
    "# val_accuracy: 0.9355\n",
    "classification_optimizer = SGD(learning_rate=model_utils.LEARNING_RATE, momentum=model_utils.MOMENTUM,\n",
    "                               nesterov=model_utils.NESTEROV_MOMENTUM)\n",
    "\n",
    "# val_accuracy: 0.9355\n",
    "classification_regularizer = None\n",
    "classification_regularizer_bias = classification_regularizer\n",
    "# val_accuracy: 0.3916\n",
    "classification_regularizer = L1L2(l1=model_utils.L1_DECAY, l2=model_utils.L2_DECAY)\n",
    "classification_regularizer_bias = classification_regularizer\n",
    "# val_accuracy: 0.8902\n",
    "classification_regularizer = L1L2(l1=0.1 * model_utils.L1_DECAY, l2=model_utils.L2_DECAY)\n",
    "classification_regularizer_bias = classification_regularizer\n",
    "\n",
    "classification_loss = 'mse'\n",
    "\n",
    "classification_model = Sequential([\n",
    "    Dense(model_utils.CLASSIFICATION_FEATURES[0], activation='gelu', input_shape=(input_size,),\n",
    "          kernel_regularizer=classification_regularizer, bias_regularizer=classification_regularizer_bias),\n",
    "    Dense(model_utils.CLASSIFICATION_FEATURES[1], activation='gelu', kernel_regularizer=classification_regularizer,\n",
    "          bias_regularizer=classification_regularizer_bias),\n",
    "    Dense(output_size, activation='gelu', kernel_regularizer=classification_regularizer,\n",
    "          bias_regularizer=classification_regularizer_bias)\n",
    "])\n",
    "classification_model.compile(optimizer=classification_optimizer, loss=classification_loss, metrics=['accuracy'])\n",
    "classification_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=model_utils.CLASSIFICATION_EPOCHS,\n",
    "                         batch_size=model_utils.BATCH_SIZE)\n",
    "\n",
    "classification_model_predict = lambda X: classification_model.predict(X)\n",
    "\n",
    "print(f'Actual number of parameters: {classification_model.count_params()}')"
   ],
   "id": "602db37c73ffde28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, _, X_train, y_train, X_test, y_test = mnist_utils.from_dataset(digits_train_32k_test_12k)\n",
    "\n",
    "print(f'Accuracy on training set: {model_utils.calculate_accuracy(classification_model_predict, X_train, y_train)}')\n",
    "print(f'Accuracy on test set: {model_utils.calculate_accuracy(classification_model_predict, X_test, y_test)}')\n",
    "\n",
    "random_test_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "print(f'Random test indices: {random_test_indices}')\n",
    "\n",
    "mnist_utils.plot_images(X_test[random_test_indices], image_size=image_size, cmap='gray')\n",
    "print(f'Actual labels of random test indices: {np.argmax(y_test[random_test_indices], axis=1)}')\n",
    "\n",
    "predicted = classification_model_predict(X_test[random_test_indices])\n",
    "print(f'Predicted labels of random test indices: {np.argmax(predicted, axis=1)}')\n",
    "predicted.round(2).astype('str')"
   ],
   "id": "7c2ea7100031339c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dimensionality Reduction with an Autoencoder\n",
    "\n",
    "This section demonstrates the use of two neural networks for dimensionality reduction. These networks rely on a bottleneck layer in their architecture to reduce the input data's dimensionality.\n",
    "\n",
    "Instead of using predefined methods to extract the most relevant features, the networks are trained using backpropagation. This approach enables the model to automatically learn a transformation into a compressed feature space along with its inverse transformation to reconstruct the input data as accurately as possible."
   ],
   "id": "14d02d0de03b9351"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Shallow Autoencoder Network with 12 Neurons Bottleneck Layer\n",
    "\n",
    "This model is configured as follows:\n",
    "\n",
    "- **Architecture**:\n",
    "  - Input layer: 784 neurons\n",
    "  - 5 hidden layers:\n",
    "    - Layer 1: 196 neurons\n",
    "    - Layer 2: 49 neurons\n",
    "    - Layer 3: 12 neurons\n",
    "    - Layer 4: 49 neurons\n",
    "    - Layer 5: 196 neurons\n",
    "  - Output layer: 784 neurons\n",
    "  - Total parameters: 328998 (Biases included)\n",
    "- **Activation Function**: Gaussian Error Linear Unit (GeLU)\n",
    "- **Optimizer**: Stochastic Gradient Descent (SGD)\n",
    "  - Learning rate: 0.02\n",
    "  - Momentum: 0.2 with Nesterov term\n",
    "  - L1-weight-decay: 0.01 * 0.001 (bigger values break training)\n",
    "  - L2-weight-decay: 0.01 * 0.001\n",
    "- **Loss Function**: Mean Squared Error (MSE)"
   ],
   "id": "33f569bb0aac8140"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T00:23:50.776697Z",
     "start_time": "2025-01-07T00:23:12.575489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size, _, X_train, _, X_test, _ = mnist_utils.from_dataset(digits_input_reconstruction)\n",
    "\n",
    "# val_ssim_metric: 0.0504\n",
    "autoencoder_optimizer = SGD(learning_rate=model_utils.BATCH_LEARNING_RATE, momentum=model_utils.MOMENTUM,\n",
    "                            nesterov=model_utils.NESTEROV_MOMENTUM)\n",
    "# val_ssim_metric: 0.4536\n",
    "autoencoder_optimizer = SGD(learning_rate=model_utils.LEARNING_RATE, momentum=model_utils.MOMENTUM,\n",
    "                            nesterov=model_utils.NESTEROV_MOMENTUM)\n",
    "\n",
    "# val_ssim_metric: 0.4536\n",
    "autoencoder_regularizer = None\n",
    "autoencoder_regularizer_bias = None\n",
    "# val_ssim_metric: 0.0980\n",
    "autoencoder_regularizer = L1L2(l1=model_utils.L1_DECAY, l2=model_utils.L2_DECAY)\n",
    "autoencoder_regularizer_bias = autoencoder_regularizer\n",
    "# val_ssim_metric: 0.4105\n",
    "autoencoder_regularizer = L1L2(l1=0.01 * model_utils.L1_DECAY, l2=0.01 * model_utils.L2_DECAY)\n",
    "autoencoder_regularizer_bias = autoencoder_regularizer\n",
    "\n",
    "autoencoder_loss = 'mse'\n",
    "\n",
    "autoencoder_model = Sequential([\n",
    "    Dense(model_utils.AUTOENCODER_FEATURES[0], activation='gelu', input_shape=(input_size,),\n",
    "          kernel_regularizer=autoencoder_regularizer, bias_regularizer=autoencoder_regularizer_bias),\n",
    "    Dense(model_utils.AUTOENCODER_FEATURES[1], activation='gelu', kernel_regularizer=autoencoder_regularizer,\n",
    "          bias_regularizer=autoencoder_regularizer_bias),\n",
    "    Dense(model_utils.AUTOENCODER_FEATURES[2], activation='gelu', kernel_regularizer=autoencoder_regularizer,\n",
    "          bias_regularizer=autoencoder_regularizer_bias),\n",
    "    Dense(model_utils.AUTOENCODER_FEATURES[3], activation='gelu', kernel_regularizer=autoencoder_regularizer,\n",
    "          bias_regularizer=autoencoder_regularizer_bias),\n",
    "    Dense(model_utils.AUTOENCODER_FEATURES[4], activation='gelu', kernel_regularizer=autoencoder_regularizer,\n",
    "          bias_regularizer=autoencoder_regularizer_bias),\n",
    "    Dense(input_size, activation='gelu', kernel_regularizer=autoencoder_regularizer,\n",
    "          bias_regularizer=autoencoder_regularizer_bias)\n",
    "])\n",
    "autoencoder_model.compile(optimizer=autoencoder_optimizer, loss=autoencoder_loss, metrics=[ssim_metric])\n",
    "autoencoder_model.fit(X_train, X_train, validation_data=(X_test, X_test), epochs=model_utils.AUTOENCODER_EPOCHS,\n",
    "                      batch_size=model_utils.BATCH_SIZE)\n",
    "\n",
    "autoencoder_model_predict = lambda X: autoencoder_model.predict(X)\n",
    "\n",
    "print(f'Actual number of parameters: {autoencoder_model.count_params()}')"
   ],
   "id": "54e122105e073306",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1500/1500 [==============================] - 13s 8ms/step - loss: 0.2556 - ssim_metric: 0.0982 - val_loss: 0.2533 - val_ssim_metric: 0.0951\n",
      "Epoch 2/1000\n",
      "1500/1500 [==============================] - 14s 9ms/step - loss: 0.2519 - ssim_metric: 0.0892 - val_loss: 0.2495 - val_ssim_metric: 0.0830\n",
      "Epoch 3/1000\n",
      "1357/1500 [==========================>...] - ETA: 1s - loss: 0.2482 - ssim_metric: 0.0773"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16476\\2060273533.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m     33\u001B[0m     Dense(input_size, activation='gelu', kernel_regularizer=autoencoder_regularizer,\n\u001B[0;32m     34\u001B[0m           bias_regularizer=autoencoder_regularizer_bias)\n\u001B[0;32m     35\u001B[0m ])\n\u001B[0;32m     36\u001B[0m \u001B[0mautoencoder_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mautoencoder_optimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mautoencoder_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mssim_metric\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m autoencoder_model.fit(X_train, X_train, validation_data=(X_test, X_test), epochs=10 * model_utils.AUTOENCODER_EPOCHS,\n\u001B[0m\u001B[0;32m     38\u001B[0m                       batch_size=model_utils.BATCH_SIZE)\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[0mautoencoder_model_predict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mlambda\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mautoencoder_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 72\u001B[1;33m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1551\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1552\u001B[0m                     data_handler._initial_step = data_handler._initial_step or (\n\u001B[0;32m   1553\u001B[0m                         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_load_initial_step_from_ckpt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1554\u001B[0m                     )\n\u001B[1;32m-> 1555\u001B[1;33m                     \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1556\u001B[0m                         with tf.profiler.experimental.Trace(\n\u001B[0;32m   1557\u001B[0m                             \u001B[1;34m\"train\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1558\u001B[0m                             \u001B[0mepoch_num\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1370\u001B[0m             \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_current_step\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_inferred_steps\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1371\u001B[0m         ):\n\u001B[0;32m   1372\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1373\u001B[0m                 \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1374\u001B[1;33m             \u001B[0moriginal_spe\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_steps_per_execution\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1375\u001B[0m             can_run_full_execution = (\n\u001B[0;32m   1376\u001B[0m                 \u001B[0moriginal_spe\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1377\u001B[0m                 \u001B[1;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_inferred_steps\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    635\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    636\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 637\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_value\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    638\u001B[0m     raise NotImplementedError(\n\u001B[0;32m    639\u001B[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    724\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Read\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m       \u001B[0mvalue\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_read_variable_op\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m     \u001B[1;31m# Return an identity so it can get placed on whatever device the context\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    727\u001B[0m     \u001B[1;31m# specifies instead of the device where the variable is.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 728\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1173\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1174\u001B[0m       \u001B[1;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1175\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1176\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1177\u001B[1;33m       \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1178\u001B[0m         \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1179\u001B[0m         \u001B[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1180\u001B[0m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m    290\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"graph\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m     \u001B[1;31m# Make sure we get an input with handle data attached from resource\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    292\u001B[0m     \u001B[1;31m# variables. Variables have correct handle data when graph building.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m     \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 294\u001B[1;33m   \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    295\u001B[0m   \u001B[1;31m# Propagate handle data for happier shape inference for resource variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    296\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_handle_data\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m     \u001B[0mret\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_handle_data\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\edu.yaprnn\\notebooks\\.venv-tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m   4069\u001B[0m         _ctx, \"Identity\", name, input)\n\u001B[0;32m   4070\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4071\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4072\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 4073\u001B[1;33m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4074\u001B[0m       \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4075\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4076\u001B[0m       return identity_eager_fallback(\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, _, X_train, _, X_test, _ = mnist_utils.from_dataset(digits_input_reconstruction)\n",
    "\n",
    "print(f'Loss X_train: {model_utils.mse_loss(autoencoder_model_predict, X_train, X_train)}')\n",
    "print(f'Loss X_test: {model_utils.mse_loss(autoencoder_model_predict, X_test, X_test)}')\n",
    "\n",
    "random_test_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "print(f'Random test indices: {random_test_indices}')\n",
    "\n",
    "original = X_test[random_test_indices]\n",
    "mnist_utils.plot_images(original, image_size=image_size, cmap='gray')\n",
    "\n",
    "restored = autoencoder_model_predict(original)\n",
    "mnist_utils.plot_images(restored, image_size=image_size, cmap='gray')"
   ],
   "id": "9d91c2218ebfb2e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
